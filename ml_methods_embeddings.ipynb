{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch.optim as optim\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "import helpers\n",
    "import torch as torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Paths\"\"\"\n",
    "POS_TWEET_PATH = 'Tweet_Large_Files/twitter-datasets/train_pos.txt'\n",
    "NEG_TWEET_PATH = \"Tweet_Large_Files/twitter-datasets/train_neg.txt\"\n",
    "VOCAB_PATH = 'vocab.pkl'\n",
    "\"\"\"change path for different glove embedding dim\"\"\"\n",
    "GLOVE_EMB_PATH = \"imported_embeddings/glove.twitter.27B.200d.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW decide which vocab to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH, 'rb') as file:\n",
    "        vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BElOW you decide which wordvector genereation to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BOW\"\"\"\n",
    "old_vectorizer = False\n",
    "fs = []\n",
    "xlen = []\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "for fn in [POS_TWEET_PATH, NEG_TWEET_PATH]:\n",
    "        with open(fn, errors= \"ignore\") as file:\n",
    "                try: \n",
    "                        fs += file.readlines()\n",
    "                        xlen.append(len(fs))\n",
    "                except:\n",
    "                        print(file.readlines())\n",
    "Ypos = np.ones(xlen[0])\n",
    "Yneg = np.ones(xlen[1]-xlen[0])*-1\n",
    "Y = np.concatenate((Ypos, Yneg))              \n",
    "X = vectorizer.fit_transform(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GloVe emdeding\"\"\"\n",
    "\"\"\"makes dir for saving embedded tweets\"\"\"\n",
    "path = 'Embedded_tweets/'\n",
    "pathExist = os.path.exists(path)\n",
    "if not pathExist:\n",
    "    os.makedirs(path)\n",
    "Xneg = helpers.build_tweet_vector(GLOVE_EMB_PATH,VOCAB_PATH,NEG_TWEET_PATH,\"Embedded_tweets/embeddings_neg_imp_dim200.npy\", TFID_weighting = False)\n",
    "print(\"pos\")\n",
    "Xpos = helpers.build_tweet_vector(GLOVE_EMB_PATH,VOCAB_PATH,POS_TWEET_PATH,\"Embedded_tweets/embeddings_pos_imp_dim200.npy\", TFID_weighting = False)\n",
    "Ypos = np.ones(Xpos.shape[0])\n",
    "Yneg = np.ones(Xneg.shape[0])*-1\n",
    "X = np.concatenate((Xneg,Xpos))\n",
    "Y = np.concatenate((Yneg, Ypos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TF-ID vectorizer\"\"\"\n",
    "old_vectorizer = False\n",
    "fs = []\n",
    "xlen = []\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "for fn in [POS_TWEET_PATH, NEG_TWEET_PATH]:\n",
    "        with open(fn, errors= \"ignore\") as file:\n",
    "                try: \n",
    "                        fs += file.readlines()\n",
    "                        xlen.append(len(fs))\n",
    "                except:\n",
    "                        print(file.readlines())\n",
    "Ypos = np.ones(xlen[0])\n",
    "Yneg = np.ones(xlen[1]-xlen[0])*-1\n",
    "Y = np.concatenate((Ypos, Yneg))              \n",
    "X = vectorizer.fit_transform(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splits training data\"\"\"\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide which supervised ML method to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multinomial naive bayes\"\"\"\n",
    "MNB = MultinomialNB()\n",
    "X_train = np.abs(X_train)\n",
    "X_test = np.abs(X_test)\n",
    "y_pred = MNB.fit(X_train, Y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"SVM\"\"\"\n",
    "SVM = LinearSVC(random_state=0, tol=1e-5, max_iter=5000)\n",
    "SVM.fit(X_train,Y_train)\n",
    "y_pred  = SVM.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Logistic regression\"\"\"\n",
    "LogReg = LogisticRegression(random_state=0,max_iter=1000).fit(X_train, Y_train)\n",
    "y_pred = LogReg.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Neural net,\n",
    "Some processing of the data need to be done before it can be trained, can only be used with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert matrices to torch tensor\"\"\"\n",
    "X_t = torch.empty((X_train.shape[0],1,X_train.shape[1]))\n",
    "\n",
    "Y_t = torch.zeros((len(Y_train)))\n",
    "for i,y in enumerate(Y_train):\n",
    "    X_t[i,0,:] = torch.from_numpy(X_train[i,:])\n",
    "    if(y == -1):\n",
    "        Y_t[i] = 0\n",
    "    else:\n",
    "        Y_t[i] = 1\n",
    "test = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "print(Y_t.type)\n",
    "print(test.type)\n",
    "X_t_test = torch.empty((X_test.shape[0],1,X_test.shape[1]))\n",
    "for i,y in enumerate(Y_test):\n",
    "    X_t_test[i,0,:] = torch.from_numpy(X_test[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 100, 2)\n",
    "        self.conv2 = nn.Conv1d(100, 100, 3)\n",
    "        self.conv3 = nn.Conv1d(100, 100, 4)\n",
    "        self.fc1 = nn.Linear(2*3*4*100-200, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.pool = nn.AvgPool1d(2, 2)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net = net.float()\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import pre existing network if you dont want to train new one, get file from google drive\"\"\"\n",
    "net.load_state_dict(torch.load(\"neural_nets/layers2_nodes128_3cnn_filters100_size4_3_2.pth\"))\n",
    "prediction =   net(X_t_test.float())\n",
    "_, y_pred = torch.max(prediction, 1)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_pred[y_pred == 0] = -1\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    permutation = torch.randperm(X_t.size()[0])\n",
    "    running_loss = 0\n",
    "    for i in range(0,X_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \"\"\"gets batch indices\"\"\"\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        \"\"\"forward and backward\"\"\"\n",
    "        outputs = net(X_t[indices].float())\n",
    "        label = Y_t[indices].long()\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        \"\"\"optimizing\"\"\"\n",
    "        optimizer.step()\n",
    "    prediction =   net(X_t_test.float())\n",
    "    _, y_pred = torch.max(prediction, 1)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    y_pred[y_pred == 0] = -1\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2740b07bebd5a9538dcad413d55b5f8f7bc0dae380b851836f69cf85491ff86f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
