{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch.optim as optim\n",
    "from sklearn.svm import LinearSVC\n",
    "import pickle\n",
    "import helpers\n",
    "import torch as torch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Paths\"\"\"\n",
    "POS_TWEET_PATH = 'twitter-datasets/train_pos.txt'\n",
    "NEG_TWEET_PATH = \"twitter-datasets/train_neg.txt\"\n",
    "VOCAB_PATH = 'vocab.pkl'\n",
    "\"\"\"change path for different glove embedding dim\"\"\"\n",
    "GLOVE_EMB_PATH = \"imported_embeddings/glove.twitter.27B.200d.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW decide which vocab to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(VOCAB_PATH, 'rb') as file:\n",
    "        vocab = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BElOW you decide which wordvector genereation to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BOW\"\"\"\n",
    "old_vectorizer = False\n",
    "fs = []\n",
    "xlen = []\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "for fn in [POS_TWEET_PATH, NEG_TWEET_PATH]:\n",
    "        with open(fn, errors= \"ignore\") as file:\n",
    "                try: \n",
    "                        fs += file.readlines()\n",
    "                        xlen.append(len(fs))\n",
    "                except:\n",
    "                        print(file.readlines())\n",
    "Ypos = np.ones(xlen[0])\n",
    "Yneg = np.ones(xlen[1]-xlen[0])*-1\n",
    "Y = np.concatenate((Ypos, Yneg))              \n",
    "X = vectorizer.fit_transform(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0\n",
      "0\n",
      "pos\n",
      "200\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"GloVe emdeding\"\"\"\n",
    "\"\"\"makes dir for saving embedded tweets\"\"\"\n",
    "path = 'Embedded_tweets/'\n",
    "pathExist = os.path.exists(path)\n",
    "if not pathExist:\n",
    "    os.makedirs(path)\n",
    "Xneg = helpers.build_tweet_vector(GLOVE_EMB_PATH,VOCAB_PATH,NEG_TWEET_PATH,\"Embedded_tweets/embeddings_neg_imp_dim200.npy\", TFID_weighting = False)\n",
    "print(\"pos\")\n",
    "Xpos = helpers.build_tweet_vector(GLOVE_EMB_PATH,VOCAB_PATH,POS_TWEET_PATH,\"Embedded_tweets/embeddings_pos_imp_dim200.npy\", TFID_weighting = False)\n",
    "Ypos = np.ones(Xpos.shape[0])\n",
    "Yneg = np.ones(Xneg.shape[0])*-1\n",
    "X = np.concatenate((Xneg,Xpos))\n",
    "Y = np.concatenate((Yneg, Ypos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"TF-ID vectorizer\"\"\"\n",
    "old_vectorizer = False\n",
    "fs = []\n",
    "xlen = []\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "for fn in [POS_TWEET_PATH, NEG_TWEET_PATH]:\n",
    "        with open(fn, errors= \"ignore\") as file:\n",
    "                try: \n",
    "                        fs += file.readlines()\n",
    "                        xlen.append(len(fs))\n",
    "                except:\n",
    "                        print(file.readlines())\n",
    "Ypos = np.ones(xlen[0])\n",
    "Yneg = np.ones(xlen[1]-xlen[0])*-1\n",
    "Y = np.concatenate((Ypos, Yneg))              \n",
    "X = vectorizer.fit_transform(fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data in train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splits training data\"\"\"\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide which supervised ML method to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 100000 points : 23652\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Multinomial naive bayes\"\"\"\n",
    "MNB = MultinomialNB()\n",
    "X_train = np.abs(X_train)\n",
    "X_test = np.abs(X_test)\n",
    "y_pred = MNB.fit(X_train, Y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 100000 points : 21989\n"
     ]
    }
   ],
   "source": [
    "\"\"\"SVM\"\"\"\n",
    "SVM = LinearSVC(random_state=0, tol=1e-5, max_iter=5000)\n",
    "SVM.fit(X_train,Y_train)\n",
    "y_pred  = SVM.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 100000 points : 21271\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Logistic regression\"\"\"\n",
    "LogReg = LogisticRegression(random_state=0,max_iter=1000).fit(X_train, Y_train)\n",
    "y_pred = LogReg.predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below Neural net,\n",
    "Some processing of the data need to be done before it can be trained, can only be used with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method type of Tensor object at 0x000002CB13AB95E0>\n",
      "<built-in method type of Tensor object at 0x000002CB5D067F90>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Convert matrices to torch tensor\"\"\"\n",
    "X_t = torch.empty((X_train.shape[0],1,X_train.shape[1]))\n",
    "\n",
    "Y_t = torch.zeros((len(Y_train)))\n",
    "for i,y in enumerate(Y_train):\n",
    "    X_t[i,0,:] = torch.from_numpy(X_train[i,:])\n",
    "    if(y == -1):\n",
    "        Y_t[i] = 0\n",
    "    else:\n",
    "        Y_t[i] = 1\n",
    "test = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "print(Y_t.type)\n",
    "print(test.type)\n",
    "X_t_test = torch.empty((X_test.shape[0],1,X_test.shape[1]))\n",
    "for i,y in enumerate(Y_test):\n",
    "    X_t_test[i,0,:] = torch.from_numpy(X_test[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 100, 2)\n",
    "        self.conv2 = nn.Conv1d(100, 100, 3)\n",
    "        self.conv3 = nn.Conv1d(100, 100, 4)\n",
    "        self.fc1 = nn.Linear(2*3*4*100-200, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.pool = nn.AvgPool1d(2, 2)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net = net.float()\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Import pre existing network if you dont want to train new one, get file from google drive\"\"\"\n",
    "net.load_state_dict(torch.load(\"neural_nets/layers2_nodes128_3cnn_filters100_size4_3_2.pth\"))\n",
    "prediction =   net(X_t_test.float())\n",
    "_, y_pred = torch.max(prediction, 1)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_pred[y_pred == 0] = -1\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14052/4079555849.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;34m\"\"\"optimizing\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m   \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14052/648510609.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    295\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 297\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    298\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    permutation = torch.randperm(X_t.size()[0])\n",
    "    running_loss = 0\n",
    "    for i in range(0,X_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \"\"\"gets batch indices\"\"\"\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        \"\"\"forward and backward\"\"\"\n",
    "        outputs = net(X_t[indices].float())\n",
    "        label = Y_t[indices].long()\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        \"\"\"optimizing\"\"\"\n",
    "        optimizer.step()\n",
    "    prediction =   net(X_t_test.float())\n",
    "    _, y_pred = torch.max(prediction, 1)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    y_pred[y_pred == 0] = -1\n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\"% (X_test.shape[0], (Y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cooccurrence matrix\n",
      "6496907 nonzero entries\n",
      "using nmax = 100 , cooc.max() = 207302\n",
      "initializing embeddings\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14052/750417119.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\sebas\\Documents\\GitHub\\ml-project-2-questforsushi\\helpers.py\u001b[0m in \u001b[0;36mglove\u001b[1;34m(embedding_dim, alpha, cooc, COOC_PATH, SAVE_PATH, with_intial, INITIAL_PATH)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjy\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mgrad_common_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlog_co\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m             \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad_common_part\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjy\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad_common_part\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "helpers.glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2740b07bebd5a9538dcad413d55b5f8f7bc0dae380b851836f69cf85491ff86f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
